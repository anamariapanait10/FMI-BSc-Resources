{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Exercitiul 1 - Scrie o func»õie care √Æmparte un text √Æn tokeni folosind regex."
      ],
      "metadata": {
        "id": "QQCoIW-1Ktp1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPNmSaczKlkO",
        "outputId": "2a0c9832-f476-4330-9e2c-534790b26c81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['We', 'don', 't', 'like', 'to', 'keep', 'our', 'lovely', 'customers', 'waiting', 'for', 'long', 'We', 'hope', 'you', 'enjoy', 'Happy', 'Friday', 'LWWF', '', 'https', 't', 'co', 'smyYriipxI']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "\n",
        "def tokenize_text(text):\n",
        "    delimiters = r\"\\s|\\W+\"\n",
        "    tokens = re.split(delimiters, text)\n",
        "    return tokens\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "tweets = twitter_samples.strings('positive_tweets.json')\n",
        "text = tweets[6]\n",
        "tokens = tokenize_text(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercitiul 2 - Scrie o func»õie care √Ænlocuie»ôte toate emoticoanele (nu doar cele date ca exemplu) »ôi emojiurile din text."
      ],
      "metadata": {
        "id": "yUACpGz7LDwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "import re\n",
        "import nltk\n",
        "import emoji\n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "\n",
        "def replace_emoticons_and_emojis(text):\n",
        "  emoticons = {\n",
        "    \"lol\": r\":\\)\\)\\)\\)+\",\n",
        "    \"happy\": r\":[\\)|D+]\",\n",
        "    \"laugh\": r\":\\)\\)+\",\n",
        "    \"very_sad\": r\":\\(\\(\\(\\(+\",\n",
        "    \"sad\": r\":\\(+\",\n",
        "  }\n",
        "\n",
        "  for emoticon, regex in emoticons.items():\n",
        "    text = re.sub(regex, f\" :{emoticon}: \", text)\n",
        "\n",
        "  emoji.demojize(text)\n",
        "\n",
        "  return text\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "tweets = twitter_samples.strings('positive_tweets.json')\n",
        "replaced_text = replace_emoticons_and_emojis(tweets[24])\n",
        "print(replaced_text)"
      ],
      "metadata": {
        "id": "C7j4hHQfKtO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6ee084-12e5-4e19-cfb8-64efa8c127f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.10.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíÖüèΩüíã -  :lol:  haven't seen you in years\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercitiul 3 - Scrie o func»õie care prime»ôte un text »ôi returneazƒÉ varianta preprocesatƒÉ. Func»õia va converti toate numerele √Æn cuvinte folosind num2words, va elimina linkurile, hashtagurile, mentions, punctua»õia, stopwords, va face toate textele literƒÉ micƒÉ »ôi va aplica lematizarea sau stemming."
      ],
      "metadata": {
        "id": "psMFmiLlLTlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install num2words\n",
        "import re\n",
        "from num2words import num2words\n",
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convertim numerele √Æn cuvinte\n",
        "    text = re.sub(r'\\b\\d+\\b', lambda x: num2words(x.group()), text)\n",
        "    # EliminƒÉm linkurile, mentions »ôi hashtagurile\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+|@[^\\s]+|#[^\\s]+', '', text)\n",
        "    # Convertim textul √Æn litere mici\n",
        "    text = text.lower()\n",
        "    # EliminƒÉm punctua»õia\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # TokenizƒÉm textul\n",
        "    words = word_tokenize(text)\n",
        "    # EliminƒÉm stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # AplicƒÉm lematizarea\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "    preprocessed_text = ' '.join(lemmatized_words)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "tweets = twitter_samples.strings('positive_tweets.json')\n",
        "preprocessed_text = preprocess_text(tweets[6])\n",
        "print(preprocessed_text)"
      ],
      "metadata": {
        "id": "qRVbMMX-L792",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530b2928-7cd3-453f-f47d-9339005b7eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: num2words in /usr/local/lib/python3.10/dist-packages (0.5.13)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from num2words) (0.6.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dont like keep lovely customer waiting long hope enjoy happy friday lwwf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercitiul 4 - AnalizeazƒÉ setul de date. UitƒÉ-te la elemente de preprocesare sau alte features folosind ce a»õi √ÆnvƒÉ»õat √Æn primul laborator. Ce pare important?"
      ],
      "metadata": {
        "id": "2zH1qrCqLYCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "tweets = twitter_samples.strings('positive_tweets.json')\n",
        "\n",
        "# Afi»ôƒÉm primele 10 tweet-uri pentru a observa caracteristicile\n",
        "for tweet in tweets[:10]:\n",
        "    print(tweet)"
      ],
      "metadata": {
        "id": "mzK52k2bL7I2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a16606be-361f-48db-bc83-d8b6ba1be14c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
            "@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
            "@97sides CONGRATS :)\n",
            "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n",
            "@BhaktisBanter @PallaviRuhail This one is irresistible :)\n",
            "#FlipkartFashionFriday http://t.co/EbZ0L2VENM\n",
            "We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\n",
            "@Impatientraider On second thought, there‚Äôs just not enough time for a DD :) But new shorts entering system. Sheep must be buying.\n",
            "Jgh , but we have to go to Bayan :D bye\n",
            "As an act of mischievousness, am calling the ETL layer of our in-house warehousing app Katamari.\n",
            "\n",
            "Well‚Ä¶ as the name implies :p.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analizand primele 10 tweet-uri, putem observa urmatoarele:\n",
        "\n",
        "1. Prezen»õa emoticoanelor pozitive (:), (:D): Emoticoanele sunt utilizate frecvent √Æn tweet-uri pentru a exprima sentimente pozitive. Acestea sunt indicatori importan»õi ai sentimentului »ôi ar trebui pƒÉstrate √Æn text pentru analiza sentimentelor.\n",
        "\n",
        "2. Men»õiuni »ôi taguri (@, #): Multe tweet-uri con»õin men»õiuni ale altor utilizatori (@) »ôi hashtaguri (#). De»ôi men»õiunile adesea nu contribuie la sentimentul general al tweet-ului »ôi ar putea fi eliminate √Æn preprocesare, hashtagurile pot fi legate de subiectele discutate »ôi ar putea fi pƒÉstrate sau tratate separat √Æn func»õie de scopul analizei.\n",
        "\n",
        "3. Linkuri: C√¢teva tweet-uri includ linkuri URL. Acestea ar putea fi eliminate deoarece nu contribuie la analiza textului.\n",
        "\n",
        "4. Textul informal »ôi abrevierile: Tweet-urile con»õin limbaj informal, abrevieri (de exemplu, \"Jgh\" - Just got home), »ôi expresii specifice re»õelelor sociale. Normalizarea acestor expresii ar putea fi necesarƒÉ, √Æn func»õie de analiza doritƒÉ.\n",
        "\n",
        "5. Men»õiuni despre evenimente sau ac»õiuni specifice: Unele tweet-uri men»õioneazƒÉ evenimente specifice (de exemplu, verificarea contului pe Facebook) sau ac»õiuni (de exemplu, apelarea unui centru de contact). Aceste detalii pot fi relevante pentru context, dar s-ar putea sƒÉ nu contribuie direct la sentimentul general al textului.\n",
        "\n",
        "6. Limbaj pozitiv »ôi entuziasm: Multe dintre tweet-uri exprimƒÉ sentimente pozitive prin cuvinte ca \"CONGRATS\", \"Happy Friday\", sau \"amazing track\". PƒÉstrarea acestor expresii este utilƒÉ pentru analiza sentimentelor.\n",
        "\n",
        "Pentru preprocesare, pe baza observa»õiilor de mai sus, o sƒÉ pƒÉstrez urmƒÉtoarele preprocesƒÉri:\n",
        "\n",
        "- PƒÉstrarea emoticoanelor\n",
        "- Eliminarea linkurilor\n",
        "- Tratarea separatƒÉ a hashtagurilor\n",
        "- Eliminarea men»õiunilor (@)\n",
        "- Normalizarea limbajului informal »ôi a abrevierilor\n"
      ],
      "metadata": {
        "id": "uOQ5VoUEl153"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercitiul 5 - Pe baza analizei anterioare scrie o func»õie de preprocesare care eliminƒÉ doar informa»õia irelevantƒÉ. Po»õi generaliza func»õia originalƒÉ specific√¢nd √Æn lista de parametri ce modificƒÉri vrei sƒÉ faci la apelarea func»õiei. Alternativ, po»õi face o clasƒÉ care include o serie de func»õii care pot fi alese la ini»õializare. Cu c√¢t e mai general cu at√¢t mai bine."
      ],
      "metadata": {
        "id": "mhg8-xEBLfyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "tweets = twitter_samples.strings('positive_tweets.json')\n",
        "\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self, remove_links=True, remove_mentions=True, remove_hashtags=True,\n",
        "                 lowercase=True, remove_punctuation=True, remove_stopwords=False,\n",
        "                 lemmatize=False, language='english'):\n",
        "        self.remove_links = remove_links\n",
        "        self.remove_mentions = remove_mentions\n",
        "        self.remove_hashtags = remove_hashtags\n",
        "        self.lowercase = lowercase\n",
        "        self.remove_punctuation = remove_punctuation\n",
        "        self.remove_stopwords = remove_stopwords\n",
        "        self.lemmatize = lemmatize\n",
        "        self.language = language\n",
        "        if remove_stopwords or lemmatize:\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('wordnet')\n",
        "            nltk.download('punkt')\n",
        "            self.stop_words = set(stopwords.words(language))\n",
        "            self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        if self.remove_links:\n",
        "            text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "        if self.remove_mentions:\n",
        "            text = re.sub(r'@\\w+', '', text)\n",
        "        if self.remove_hashtags:\n",
        "            text = re.sub(r'#\\w+', '', text)\n",
        "        if self.lowercase:\n",
        "            text = text.lower()\n",
        "        if self.remove_punctuation:\n",
        "            text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        if self.remove_stopwords or self.lemmatize:\n",
        "            words = word_tokenize(text)\n",
        "            if self.remove_stopwords:\n",
        "                words = [word for word in words if word not in self.stop_words]\n",
        "            if self.lemmatize:\n",
        "                words = [self.lemmatizer.lemmatize(word) for word in words]\n",
        "            text = ' '.join(words)\n",
        "        return text\n",
        "\n",
        "preprocessor = TextPreprocessor(remove_links=True, remove_mentions=True, remove_hashtags=False,\n",
        "                                lowercase=True, remove_punctuation=True, remove_stopwords=True,\n",
        "                                lemmatize=True)\n",
        "\n",
        "\n",
        "text = tweets[6]\n",
        "preprocessed_text = preprocessor.preprocess(text)\n",
        "print(\"-\"*40)\n",
        "print(preprocessed_text)\n",
        "print(\"-\"*40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uv4bcMkeluSl",
        "outputId": "d3c3f01d-9588-4c39-b260-897f9300fcdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "dont like keep lovely customer waiting long hope enjoy happy friday lwwf\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercitiul 6 - ComparƒÉ metodele de preprocesare."
      ],
      "metadata": {
        "id": "6kiV_U9JLobb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pentru analiza sentimentelor √Æn tweet-uri, pƒÉstrarea emoticoanelor »ôi eliminarea informa»õiilor neesen»õiale precum linkurile »ôi men»õiunile sunt pa»ôi critici, √Æn timp ce normalizarea limbajului poate juca un rol cheie √Æn √ÆmbunƒÉtƒÉ»õirea preciziei analizei. Experimentarea »ôi evaluarea impactului diferitelor metode de preprocesare asupra setului de date specific sunt esen»õiale pentru optimizarea procesului de analizƒÉ."
      ],
      "metadata": {
        "id": "trKyHqEMmTWH"
      }
    }
  ]
}